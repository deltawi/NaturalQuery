{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmlingua\n",
      "  Downloading llmlingua-0.1.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: nltk in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from llmlingua) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from llmlingua) (1.26.2)\n",
      "Requirement already satisfied: tiktoken in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from llmlingua) (0.5.2)\n",
      "Requirement already satisfied: torch in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from llmlingua) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from llmlingua) (4.36.2)\n",
      "Requirement already satisfied: filelock in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from transformers>=4.26.0->llmlingua) (4.66.1)\n",
      "Requirement already satisfied: click in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from nltk->llmlingua) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from nltk->llmlingua) (1.3.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from torch->llmlingua) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from torch->llmlingua) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from torch->llmlingua) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from torch->llmlingua) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from torch->llmlingua) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from requests->transformers>=4.26.0->llmlingua) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from jinja2->torch->llmlingua) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/mehditantaoui/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages (from sympy->torch->llmlingua) (1.3.0)\n",
      "Downloading llmlingua-0.1.5-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: llmlingua\n",
      "Successfully installed llmlingua-0.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq\n",
      "  Using cached auto_gptq-0.6.0.tar.gz (120 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/kg/fmxh64rs0c781kf4dqw14h3r0000gn/T/pip-install-5u6o5jua/auto-gptq_23f7b9d710b047f29de367b46a3bebf6/setup.py\", line 60, in <module>\n",
      "  \u001b[31m   \u001b[0m     CUDA_VERSION = \"\".join(os.environ.get(\"CUDA_VERSION\", default_cuda_version).split(\".\"))\n",
      "  \u001b[31m   \u001b[0m AttributeError: 'NoneType' object has no attribute 'split'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 789/789 [00:00<00:00, 2.72MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 1.28MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:01<00:00, 358kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:13<00:00, 139kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 608kB/s]\n",
      "config.json: 100%|██████████| 789/789 [00:00<00:00, 1.64MB/s]\n"
     ]
    },
    {
     "ename": "PackageNotFoundError",
     "evalue": "auto-gptq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllmlingua\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptCompressor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#llm_lingua = PromptCompressor()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m## Before that, you need to pip install optimum auto-gptq\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m llm_lingua \u001b[38;5;241m=\u001b[39m \u001b[43mPromptCompressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheBloke/Llama-2-7b-Chat-GPTQ\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:27\u001b[0m, in \u001b[0;36mPromptCompressor.__init__\u001b[0;34m(self, model_name, device_map, model_config, open_api_config)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     22\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNousResearch/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     open_api_config: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     26\u001b[0m ):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages/llmlingua/prompt_compressor.py:66\u001b[0m, in \u001b[0;36mPromptCompressor.load_model\u001b[0;34m(self, model_name, device_map, model_config)\u001b[0m\n\u001b[1;32m     57\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     58\u001b[0m         model_name,\n\u001b[1;32m     59\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config,\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/offload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Challenges/GenAI-SQL/NaturalQuery/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:3013\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   3005\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `quantization_config` to `from_pretrained` but the model you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre loading already has a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3006\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`quantization_config` attribute and has already quantized weights. However, loading attributes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3007\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (e.g. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(loading_attr_dict\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3008\u001b[0m     )\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3010\u001b[0m     quantization_method_from_args \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m quantization_method_from_config \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ\n\u001b[1;32m   3012\u001b[0m ):\n\u001b[0;32m-> 3013\u001b[0m     gptq_supports_cpu \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto-gptq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.4.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gptq_supports_cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m   3015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU is required to quantize or run quantize model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/metadata.py:569\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m    563\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/metadata.py:542\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/metadata.py:196\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: auto-gptq"
     ]
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "#llm_lingua = PromptCompressor()\n",
    "\n",
    "# > {'compressed_prompt': 'Question: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He reanged five of boxes into packages of sixlters each and sold them $3 per. He sold the rest theters separately at the of three pens $2. How much did make in total, dollars?\\nLets think step step\\nSam bought 1 boxes x00 oflters.\\nHe bought 12 * 300ters in total\\nSam then took 5 boxes 6ters0ters.\\nHe sold these boxes for 5 *5\\nAfterelling these  boxes there were 3030 highlighters remaining.\\nThese form 330 / 3 = 110 groups of three pens.\\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\\nIn total, then, he earned $220 + $15 = $235.\\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\\nThe answer is 115',\n",
    "#  'origin_tokens': 2365,\n",
    "#  'compressed_tokens': 211,\n",
    "#  'ratio': '11.2x',\n",
    "#  'saving': ', Saving $0.1 in GPT-4.'}\n",
    "\n",
    "## Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\n",
    "## Before that, you need to pip install optimum auto-gptq\n",
    "#llm_lingua = PromptCompressor(\"TheBloke/Llama-2-7B-Chat-GGUF/\", model_config={\"revision\": \"main\"}, device_map='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
